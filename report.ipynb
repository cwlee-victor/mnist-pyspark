{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import nessecary modules and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "seed = 56611230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://test-laptop:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e4fd859b70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"label\", IntegerType(), True)] +\n",
    "                    [StructField(f\"pixel{i}\", IntegerType(), True) for i in range(1, 785)])\n",
    "\n",
    "mnist_train = spark.read.options(header = False).schema(schema).csv(\"dataset/mnist_train.csv.tar.gz\").dropna()\n",
    "mnist_test = spark.read.options(header = False).schema(schema).csv(\"dataset/mnist_test.csv.tar.gz\").dropna()\n",
    "\n",
    "# It is recommended to unzip the .csv\n",
    "\n",
    "# mnist_train = spark.read.options(header = False).schema(schema).csv(\"dataset/mnist_train.csv\").dropna()\n",
    "# mnist_test = spark.read.options(header = False).schema(schema).csv(\"dataset/mnist_test.csv\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat the train and test set<br/>\n",
    "\n",
    "These images are flattened into a 784-dimensional vector (28*28) where each component represents the grayscale intensity of a pixel to be the input for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[f\"pixel{i}\" for i in range(1, 785)],\n",
    "                            outputCol=\"features\")\n",
    "train = assembler.transform(mnist_train)\n",
    "test = assembler.transform(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic regression<br/>\n",
    "\n",
    "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable. It is a form of binomial regression.<br/>\n",
    "\n",
    "$$\n",
    "p(y = 1 | \\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^T\\mathbf{x} - b}}\n",
    "$$\n",
    "\n",
    "The core of logistic regression is the logistic function, where z is the linear combination of the input features x with their corresponding weights w and bias b. It is estimated using maximum likelihood estimation to find the best parameters (weights and bias) that maximize the likelihood of producing the observed set of data.<br/>\n",
    "\n",
    "Logistic regression output for MNIST is a vector of probabilities due to the multi-class nature of the dataset (digits 0 through 9).<br/>\n",
    "\n",
    "Below shows the result for logistic regression, which shows a quite-good performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (Logistic regression) = 0.8965833333333333\n",
      "Test Accuracy (Logistic regression) = 0.9002\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "train_predictions = lr_model.transform(train)\n",
    "train_accuracy = evaluator.evaluate(train_predictions)\n",
    "test_predictions = lr_model.transform(test)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Training Accuracy (Logistic regression) = {train_accuracy}\")\n",
    "print(f\"Test Accuracy (Logistic regression) = {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regParam : This controls the regularization strength, which helps to prevent overfitting. A higher value increases regularization strength, which reduce overfitting but may underfit the data.<br/>\n",
    "\n",
    "elasticNetParam: Dictates the mix between L1 and L2 regularization. A value of 0 corresponds to L2 regularization only, while 1 corresponds to L1 only.<br/>\n",
    "\n",
    "Below shows an example for logistic regression with regularization. A higher regularization will reduce the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (Logistic regression) = 0.88075\n",
      "Test Accuracy (Logistic regression) = 0.8884\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10, regParam=0.01, elasticNetParam=0.5)\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "train_predictions = lr_model.transform(train)\n",
    "train_accuracy = evaluator.evaluate(train_predictions)\n",
    "test_predictions = lr_model.transform(test)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Training Accuracy (Logistic regression) = {train_accuracy}\")\n",
    "print(f\"Test Accuracy (Logistic regression) = {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Mean clustering<br/>\n",
    "\n",
    "K-Means is a clustering algorithm that partitions a given dataset into a predefined number of clusters, denoted as k, that the points in the same cluster are more similar to each other than to those in other clusters.<br/>\n",
    "\n",
    "Assignment step: Each observation is assigned to the nearest cluster by minimizing the distance between the observation and the cluster centroid.<br/>\n",
    "\n",
    "$$\n",
    "S_i^{(t)} = \\{ x_p : \\| x_p - \\mu_i^{(t)} \\| \\leq \\| x_p - \\mu_j^{(t)} \\| \\text{ for all } j = 1, \\dots, k \\}\n",
    "$$\n",
    "\n",
    "Update step: The centroids of the clusters are recalculated as the mean of all observations assigned to each cluster:<br/>\n",
    "\n",
    "$$\n",
    "\\mu_i^{(t+1)} = \\frac{1}{|S_i^{(t)}|} \\sum_{x_j \\in S_i^{(t)}} x_j\n",
    "$$\n",
    "\n",
    "The output of applying K-Means to MNIST would be the assignment of each image to one of k clusters.<br/>\n",
    "\n",
    "Ideally, we can only choose k=10 as we have a fixed number of 10 output for MNIST.<br/>\n",
    "\n",
    "The Silhouette Score is a popular metric for assessing the quality of clusters created by algorithms like K-means. It measures how similar each data point is to its own cluster compared to other clusters.<br/>\n",
    "\n",
    "Below shows the result for K-Mean clustering. These scores are close to 0, which suggests that the clusters are overlapping quite a bit and are not distinctly separated, implying that K-Mean clustering may not be a good idea for MNIST problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Silhouette Score (K-means) = 0.10656523514439634\n",
      "Test Silhouette Score (K-means) = 0.1083833467339384\n"
     ]
    }
   ],
   "source": [
    "# Train a K-means model\n",
    "kmeans = KMeans().setK(10).setSeed(seed)\n",
    "kmeans_model = kmeans.fit(train.select('features'))  # Note: no labels used\n",
    "\n",
    "# Assign clusters\n",
    "train_predictions_kmeans = kmeans_model.transform(train)\n",
    "test_predictions_kmeans = kmeans_model.transform(test)\n",
    "\n",
    "# Evaluate clustering\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette_train = evaluator.evaluate(train_predictions_kmeans)\n",
    "silhouette_test = evaluator.evaluate(test_predictions_kmeans)\n",
    "\n",
    "print(f\"Train Silhouette Score (K-means) = {silhouette_train}\")\n",
    "print(f\"Test Silhouette Score (K-means) = {silhouette_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer perceptron (MLP)<br/>\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a type of artificial neural network for tackling complex prediction tasks. MLPs belong to the category of feedforward neural networks, which means the data flows in one direction from input to output, with no cycles or loops.<br/>\n",
    "\n",
    "An MLP consists of multiple layers, each composed of nodes or neurons.<br/>\n",
    "\n",
    "Input Layer: This layer receives the raw input signal.<br/>\n",
    "Hidden Layers: One or more layers that perform computations and feature transformations.<br/>\n",
    "Output Layer: This layer produces the final prediction or classification output.<br/>\n",
    "\n",
    "Node:<br/>\n",
    "\n",
    "$$\n",
    "a_i = \\sigma\\left(\\sum_{j} w_{ij} x_j + b_i\\right)\n",
    "$$\n",
    "\n",
    "Cost function:<br/>\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Backpropagation:<br/>\n",
    "\n",
    "$$\n",
    "w_{ij} \\leftarrow w_{ij} - \\eta \\frac{\\partial C}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "The output layer consists of 10 neurons, where each neuron corresponds to a digit from 0 to 9.\n",
    "\n",
    "Below shows the result for MLPs. 2 types of architecture are tried, and both attatined a simialr result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (MLP) = 0.9563166666666667\n",
      "Test Accuracy (MLP) = 0.9446\n"
     ]
    }
   ],
   "source": [
    "layers = [784, 128, 64, 10]  # 784 input features, two hidden layers of 128 and 64 neurons, and 10 output classes\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=seed)\n",
    "\n",
    "mlp_model = mlp.fit(train)\n",
    "\n",
    "train_predictions_mlp = mlp_model.transform(train)\n",
    "test_predictions_mlp = mlp_model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "train_accuracy_mlp = evaluator.evaluate(train_predictions_mlp)\n",
    "test_accuracy_mlp = evaluator.evaluate(test_predictions_mlp)\n",
    "\n",
    "print(f\"Training Accuracy (MLP) = {train_accuracy_mlp}\")\n",
    "print(f\"Test Accuracy (MLP) = {test_accuracy_mlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (MLP) = 0.9568\n",
      "Test Accuracy (MLP) = 0.9423\n"
     ]
    }
   ],
   "source": [
    "layers = [784, 256, 128, 64, 10]  # 784 input features, three hidden layers of 256, 128 and 64 neurons, and 10 output classes\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=seed)\n",
    "\n",
    "mlp_model = mlp.fit(train)\n",
    "\n",
    "train_predictions_mlp = mlp_model.transform(train)\n",
    "test_predictions_mlp = mlp_model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "train_accuracy_mlp = evaluator.evaluate(train_predictions_mlp)\n",
    "test_accuracy_mlp = evaluator.evaluate(test_predictions_mlp)\n",
    "\n",
    "print(f\"Training Accuracy (MLP) = {train_accuracy_mlp}\")\n",
    "print(f\"Test Accuracy (MLP) = {test_accuracy_mlp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result<br/>\n",
    "\n",
    "We can see MLP is having the best result among all methods. Although we cannot compare the result of K-Mean clustering with other models, we can still see that logistic regression is possibly outperform K-Mean clustering due to its fantastic performance comparing to K-Mean clustering.<br/>\n",
    "\n",
    "#### Logistic Regression<br/>\n",
    "\n",
    "Advantages:<br/>\n",
    "\n",
    "It is relatively simple to implement, understand, and interpret.<br/>\n",
    "It is computationally less intensive than more complex models like MLP.<br/>\n",
    "It provides probabilities for outcomes, which can be a useful measure of confidence in predictions.<br/>\n",
    "\n",
    "Disadvantages:<br/>\n",
    "\n",
    "It can only capture linear boundaries between classes unless manually extended with kernels or polynomial terms.<br/>\n",
    "\n",
    "#### K-means Clustering<br/>\n",
    "\n",
    "Advantages:<br/>\n",
    "\n",
    "It can be used to find patterns or groupings in data without needing any labels.<br/>\n",
    "It is typically fast and efficient in terms of computational resources.<br/>\n",
    "It can be used for feature extraction or dimensionality reduction before applying another classification technique.<br/>\n",
    "\n",
    "Disadvantages:<br/>\n",
    "\n",
    "It does not utilize label information, making it less suitable directly for classification tasks like MNIST.<br/>\n",
    "It assumes clusters are spherical and equally sized, which might not hold true for complex datasets.<br/>\n",
    "The results can significantly vary based on the initial cluster centers' placement.<br/>\n",
    "\n",
    "#### Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Advantages:<br/>\n",
    "\n",
    "It can model highly complex relationships due to its capability to learn nonlinear models.<br/>\n",
    "\n",
    "It performs well on large and complex datasets like MNIST.<br/>\n",
    "It is capable of learning feature interactions automatically without needing manual intervention.<br/>\n",
    "\n",
    "Disadvantages:<br/>\n",
    "\n",
    "It is more computationally intensive and requires more resources, making training longer and more expensive.<br/>\n",
    "Without proper regularization, MLPs can easily overfit to training data.<br/>\n",
    "It requires careful tuning of parameters, including the number of hidden layers, number of neurons in each layer, learning rate, etc.<br/>\n",
    "\n",
    "#### Comparison:\n",
    "\n",
    "MLP: Best performance due to its flexibility in modeling non-linear and high dimensional data.<br/>\n",
    "Logistic Regression: Decent performance but limited by its linearity.<br/>\n",
    "K-means: Least suitable for direct application to classification problems like MNIST due to its unsupervised nature and basic assumptions about data distribution.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
